{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCcFxBMNgFMwLqwUS4XQhP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["4 a)\n","\n","$\\nabla_W \\ell(W)$$=\\sum_{i=1}^n(\\nabla_W\\log|W|+\\sum_{i=1}^d\\nabla_W\\log g'(w_j^Tx^{(i)}))$$=\\sum_{i=1}^n(\\frac{1}{|W|}|W|(W^{-1})^T+\\sum_{j=1}^d \\nabla_W (\\log (\\frac{1}{\\sqrt{2\\pi}}e^{-(w_j^Tx^{(i)})^2/2}))$$=\\sum_{i=1}^n ((W^{-1})^T+\\sum_{j=1}^d\\nabla_W (-(w_j^Tx^{(i)})^2/2))$$=\\sum_{i=1}^n ((W^{-1})^T-\\frac{1}{2}\\sum_{j=1}^d\\nabla_W(w_j^Tx^{(i)})^2)$$=\\sum_{i=1}^n ((W^{-1})^T-\\frac{1}{2}\\sum_{j=1}^d 2(w_j^Tx^{(i)})\\nabla_W(w_j^Tx^{(i)})$$=\\sum_{i=1}^n((W^{-1})^T-(W x^{(i)})(x^{(i)})^T$$=n (W^{-1})^T-WX^T X$.\n","\n","Setting $\\nabla_W\\ell(W)=0$ gives $n(W^{-1})^T-WX^TX=0$, $n(W^{-1})^T=WX^TX$, $nW^{-1}=X^TXW^T$, and $W^TW=n(X^TX)^{-1}$. Given an orthonormal matrix $A$, $W^TA^TAW=W^TW$. So, the above equation couldn't uniquely determine $W$.\n","\n","4 b)\n","\n","$\\nabla_W\\ell(W)$$=\\nabla_W (\\sum_{i=1}^n (\\log |W| + \\sum_{j=1}^d \\log (\\frac{1}{2}exp(-|w_j^Tx^{(i)}|)))$$=\\nabla_W(\\sum_{i=1}^n \\log |W|+\\sum_{j=1}^d -|w_j^Tx^{(i)}|)$$=n (W^{-1})^T+\\sum_{i=1}^n\\sum_{j=1}^d \\nabla_W-|w_j^Tx^{(i)}|$$=n(W^{-1})^T+\\sum_{i=1}^n\\sum_{j=1}^d-sgn(w^T_j x^{(i)})(x^{(i)})_j$ with $(x^{(i)})_j$ denoting the matrix with a single nonzero $j$th row being $x^{(i)}$. To maximize $\\ell(W)$, set update to be $\\nabla_W\\ell(W)$ as above.\n","\n","4 c)\n"],"metadata":{"id":"TQdDDORefS6l"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JPV3owo4fQhz","executionInfo":{"status":"ok","timestamp":1723010038038,"user_tz":-480,"elapsed":36097,"user":{"displayName":"Henry Z","userId":"04217387948606298563"}},"outputId":"4671e803-52fb-41e0-ccc7-c082e0727e31"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","(53442, 5)\n","Separating tracks ...\n","0.1\n","0.1\n","0.1\n","0.05\n","0.05\n","0.05\n","0.02\n","0.02\n","0.01\n","0.01\n","0.005\n","0.005\n","0.002\n","0.002\n","0.001\n","0.001\n","[[ 43.87436024  14.20333802  15.87371503  -9.14085174 -15.73432699]\n"," [ 15.05431262  26.69048344   3.35867168 -10.19254642  11.41007428]\n"," [ 14.83783755   2.31603796  20.13501041   8.39318277  -5.09756764]\n"," [ -9.95657858 -10.0089948    7.20614667  23.32738889   2.19641418]\n"," [-12.79418218  12.89666462  -3.00203513   2.47665334  33.20130964]]\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import numpy as np\n","import scipy.io.wavfile\n","import os\n","import numpy as np\n","\n","def update_W(W, x, learning_rate):\n","    \"\"\"\n","    Perform a gradient ascent update on W using data element x and the provided learning rate.\n","    This function should return the updated W.\n","    Use the laplace distribiution in this problem.\n","    Args:\n","        W: The W matrix for ICA\n","        x: A single data element\n","        learning_rate: The learning rate to use\n","    Returns:\n","        The updated W\n","    \"\"\"\n","    # *** START CODE HERE ***\n","    \"\"\"\n","    Likelihood of a single data element x equals \\log |W| + \\sum_{j=1}^d \\log g'(w_j^Tx^{(i)})\n","    stochastic gradient ascent update equals (W^{-1})^T + \\sum_{j=1}^d -sgn(w_j^T x^{(i)}) (x^{(i)})_j\n","    \"\"\"\n","    update = np.zeros(W.shape)\n","    update += np.linalg.inv(W)\n","    signs = np.sign(W @ x)\n","    update += - signs[:, None] * x[None, :]\n","    updated_W = W + learning_rate * update\n","    # *** END CODE HERE ***\n","    return updated_W\n","\n","def unmix(X, W):\n","    \"\"\"\n","    Unmix an X matrix according to W using ICA.\n","    Args:\n","        X: The data matrix\n","        W: The W for ICA\n","    Returns:\n","        A numpy array S containing the split data\n","    \"\"\"\n","    # *** START CODE HERE ***\n","    S = (W @ X.T).T\n","    # *** END CODE HERE ***\n","    return S\n","\n","Fs = 11025\n","\n","def normalize(dat):\n","    return 0.99 * dat / np.max(np.abs(dat))\n","\n","def load_data():\n","    mix = np.loadtxt('/content/drive/MyDrive/cs229/problem-sets/PS4/data/mix.dat')\n","    return mix\n","\n","def save_W(W):\n","    np.savetxt('/content/drive/MyDrive/cs229/problem-sets/PS4/output/W.txt',W)\n","\n","def save_sound(audio, name):\n","    scipy.io.wavfile.write('/content/drive/MyDrive/cs229/problem-sets/PS4/output/{}.wav'.format(name), Fs, audio)\n","\n","def unmixer(X):\n","    M, N = X.shape\n","    W = np.eye(N)\n","    anneal = [0.1 , 0.1, 0.1, 0.05, 0.05, 0.05, 0.02, 0.02, 0.01 , 0.01, 0.005, 0.005, 0.002, 0.002, 0.001, 0.001]\n","    print('Separating tracks ...')\n","    for lr in anneal:\n","        print(lr)\n","        rand = np.random.permutation(range(M))\n","        for i in rand:\n","            x = X[i]\n","            W = update_W(W, x, lr)\n","    return W\n","\n","def main():\n","    # Seed the randomness of the simulation so this outputs the same thing each time\n","    np.random.seed(0)\n","    X = normalize(load_data())\n","    print(X.shape)\n","    for i in range(X.shape[1]):\n","        save_sound(X[:, i], 'mixed_{}'.format(i))\n","    W = unmixer(X)\n","    print(W)\n","    save_W(W)\n","    S = normalize(unmix(X, W))\n","    assert S.shape[1] == 5\n","    for i in range(S.shape[1]):\n","        if os.path.exists('split_{}'.format(i)):\n","            os.unlink('split_{}'.format(i))\n","        save_sound(S[:, i], 'split_{}'.format(i))\n","\n","if __name__ == '__main__':\n","    main()"]}]}