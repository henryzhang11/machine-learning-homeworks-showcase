{"cells":[{"cell_type":"markdown","metadata":{"id":"tZHr9tpnRTau"},"source":["1"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"XTl9gyLQRQ5n","executionInfo":{"status":"ok","timestamp":1722860854587,"user_tz":-480,"elapsed":204351,"user":{"displayName":"Henry Z","userId":"04217387948606298563"}},"outputId":"d172d68d-7954-4ce3-a3de-92e7aa0f9ef6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Currently processing 0 / 400\n","Cost and accuracy 2.5063406691816548 0.1175\n","Currently processing 1 / 400\n","Currently processing 2 / 400\n","Currently processing 3 / 400\n","Currently processing 4 / 400\n","Currently processing 5 / 400\n","Currently processing 6 / 400\n","Currently processing 7 / 400\n","Currently processing 8 / 400\n","Currently processing 9 / 400\n","Currently processing 10 / 400\n","Currently processing 11 / 400\n","Currently processing 12 / 400\n","Currently processing 13 / 400\n","Currently processing 14 / 400\n","Currently processing 15 / 400\n","Currently processing 16 / 400\n","Currently processing 17 / 400\n","Currently processing 18 / 400\n","Currently processing 19 / 400\n","Currently processing 20 / 400\n","Currently processing 21 / 400\n","Currently processing 22 / 400\n","Currently processing 23 / 400\n","Currently processing 24 / 400\n","Currently processing 25 / 400\n","Currently processing 26 / 400\n","Currently processing 27 / 400\n","Currently processing 28 / 400\n","Currently processing 29 / 400\n","Currently processing 30 / 400\n","Currently processing 31 / 400\n","Currently processing 32 / 400\n","Currently processing 33 / 400\n","Currently processing 34 / 400\n","Currently processing 35 / 400\n","Currently processing 36 / 400\n","Currently processing 37 / 400\n","Currently processing 38 / 400\n","Currently processing 39 / 400\n","Currently processing 40 / 400\n","Currently processing 41 / 400\n","Currently processing 42 / 400\n","Currently processing 43 / 400\n","Currently processing 44 / 400\n","Currently processing 45 / 400\n","Currently processing 46 / 400\n","Currently processing 47 / 400\n","Currently processing 48 / 400\n","Currently processing 49 / 400\n","Currently processing 50 / 400\n","Currently processing 51 / 400\n","Currently processing 52 / 400\n","Currently processing 53 / 400\n","Currently processing 54 / 400\n","Currently processing 55 / 400\n","Currently processing 56 / 400\n","Currently processing 57 / 400\n","Currently processing 58 / 400\n","Currently processing 59 / 400\n","Currently processing 60 / 400\n","Currently processing 61 / 400\n","Currently processing 62 / 400\n","Currently processing 63 / 400\n","Currently processing 64 / 400\n","Currently processing 65 / 400\n","Currently processing 66 / 400\n","Currently processing 67 / 400\n","Currently processing 68 / 400\n","Currently processing 69 / 400\n","Currently processing 70 / 400\n","Currently processing 71 / 400\n","Currently processing 72 / 400\n","Currently processing 73 / 400\n","Currently processing 74 / 400\n","Currently processing 75 / 400\n","Currently processing 76 / 400\n","Currently processing 77 / 400\n","Currently processing 78 / 400\n","Currently processing 79 / 400\n","Currently processing 80 / 400\n","Currently processing 81 / 400\n","Currently processing 82 / 400\n","Currently processing 83 / 400\n","Currently processing 84 / 400\n","Currently processing 85 / 400\n","Currently processing 86 / 400\n","Currently processing 87 / 400\n","Currently processing 88 / 400\n","Currently processing 89 / 400\n","Currently processing 90 / 400\n","Currently processing 91 / 400\n","Currently processing 92 / 400\n","Currently processing 93 / 400\n","Currently processing 94 / 400\n","Currently processing 95 / 400\n","Currently processing 96 / 400\n","Currently processing 97 / 400\n","Currently processing 98 / 400\n","Currently processing 99 / 400\n","Currently processing 100 / 400\n","Cost and accuracy 1.2293170388320391 0.635\n","Currently processing 101 / 400\n","Currently processing 102 / 400\n","Currently processing 103 / 400\n","Currently processing 104 / 400\n","Currently processing 105 / 400\n","Currently processing 106 / 400\n","Currently processing 107 / 400\n","Currently processing 108 / 400\n","Currently processing 109 / 400\n","Currently processing 110 / 400\n","Currently processing 111 / 400\n","Currently processing 112 / 400\n","Currently processing 113 / 400\n","Currently processing 114 / 400\n","Currently processing 115 / 400\n","Currently processing 116 / 400\n","Currently processing 117 / 400\n","Currently processing 118 / 400\n","Currently processing 119 / 400\n","Currently processing 120 / 400\n","Currently processing 121 / 400\n","Currently processing 122 / 400\n","Currently processing 123 / 400\n","Currently processing 124 / 400\n","Currently processing 125 / 400\n","Currently processing 126 / 400\n","Currently processing 127 / 400\n","Currently processing 128 / 400\n","Currently processing 129 / 400\n","Currently processing 130 / 400\n","Currently processing 131 / 400\n","Currently processing 132 / 400\n","Currently processing 133 / 400\n","Currently processing 134 / 400\n","Currently processing 135 / 400\n","Currently processing 136 / 400\n","Currently processing 137 / 400\n","Currently processing 138 / 400\n","Currently processing 139 / 400\n","Currently processing 140 / 400\n","Currently processing 141 / 400\n","Currently processing 142 / 400\n","Currently processing 143 / 400\n","Currently processing 144 / 400\n","Currently processing 145 / 400\n","Currently processing 146 / 400\n","Currently processing 147 / 400\n","Currently processing 148 / 400\n","Currently processing 149 / 400\n","Currently processing 150 / 400\n","Currently processing 151 / 400\n","Currently processing 152 / 400\n","Currently processing 153 / 400\n","Currently processing 154 / 400\n","Currently processing 155 / 400\n","Currently processing 156 / 400\n","Currently processing 157 / 400\n","Currently processing 158 / 400\n","Currently processing 159 / 400\n","Currently processing 160 / 400\n","Currently processing 161 / 400\n","Currently processing 162 / 400\n","Currently processing 163 / 400\n","Currently processing 164 / 400\n","Currently processing 165 / 400\n","Currently processing 166 / 400\n","Currently processing 167 / 400\n","Currently processing 168 / 400\n","Currently processing 169 / 400\n","Currently processing 170 / 400\n","Currently processing 171 / 400\n","Currently processing 172 / 400\n","Currently processing 173 / 400\n","Currently processing 174 / 400\n","Currently processing 175 / 400\n","Currently processing 176 / 400\n","Currently processing 177 / 400\n","Currently processing 178 / 400\n","Currently processing 179 / 400\n","Currently processing 180 / 400\n","Currently processing 181 / 400\n","Currently processing 182 / 400\n","Currently processing 183 / 400\n","Currently processing 184 / 400\n","Currently processing 185 / 400\n","Currently processing 186 / 400\n","Currently processing 187 / 400\n","Currently processing 188 / 400\n","Currently processing 189 / 400\n","Currently processing 190 / 400\n","Currently processing 191 / 400\n","Currently processing 192 / 400\n","Currently processing 193 / 400\n","Currently processing 194 / 400\n","Currently processing 195 / 400\n","Currently processing 196 / 400\n","Currently processing 197 / 400\n","Currently processing 198 / 400\n","Currently processing 199 / 400\n","Currently processing 200 / 400\n","Cost and accuracy 0.972017060844784 0.6875\n","Currently processing 201 / 400\n","Currently processing 202 / 400\n","Currently processing 203 / 400\n","Currently processing 204 / 400\n","Currently processing 205 / 400\n","Currently processing 206 / 400\n","Currently processing 207 / 400\n","Currently processing 208 / 400\n","Currently processing 209 / 400\n","Currently processing 210 / 400\n","Currently processing 211 / 400\n","Currently processing 212 / 400\n","Currently processing 213 / 400\n","Currently processing 214 / 400\n","Currently processing 215 / 400\n","Currently processing 216 / 400\n","Currently processing 217 / 400\n","Currently processing 218 / 400\n","Currently processing 219 / 400\n","Currently processing 220 / 400\n","Currently processing 221 / 400\n","Currently processing 222 / 400\n","Currently processing 223 / 400\n","Currently processing 224 / 400\n","Currently processing 225 / 400\n","Currently processing 226 / 400\n","Currently processing 227 / 400\n","Currently processing 228 / 400\n","Currently processing 229 / 400\n","Currently processing 230 / 400\n","Currently processing 231 / 400\n","Currently processing 232 / 400\n","Currently processing 233 / 400\n","Currently processing 234 / 400\n","Currently processing 235 / 400\n","Currently processing 236 / 400\n","Currently processing 237 / 400\n","Currently processing 238 / 400\n","Currently processing 239 / 400\n","Currently processing 240 / 400\n","Currently processing 241 / 400\n","Currently processing 242 / 400\n","Currently processing 243 / 400\n","Currently processing 244 / 400\n","Currently processing 245 / 400\n","Currently processing 246 / 400\n","Currently processing 247 / 400\n","Currently processing 248 / 400\n","Currently processing 249 / 400\n","Currently processing 250 / 400\n","Currently processing 251 / 400\n","Currently processing 252 / 400\n","Currently processing 253 / 400\n","Currently processing 254 / 400\n","Currently processing 255 / 400\n","Currently processing 256 / 400\n","Currently processing 257 / 400\n","Currently processing 258 / 400\n","Currently processing 259 / 400\n","Currently processing 260 / 400\n","Currently processing 261 / 400\n","Currently processing 262 / 400\n","Currently processing 263 / 400\n","Currently processing 264 / 400\n","Currently processing 265 / 400\n","Currently processing 266 / 400\n","Currently processing 267 / 400\n","Currently processing 268 / 400\n","Currently processing 269 / 400\n","Currently processing 270 / 400\n","Currently processing 271 / 400\n","Currently processing 272 / 400\n","Currently processing 273 / 400\n","Currently processing 274 / 400\n","Currently processing 275 / 400\n","Currently processing 276 / 400\n","Currently processing 277 / 400\n","Currently processing 278 / 400\n","Currently processing 279 / 400\n","Currently processing 280 / 400\n","Currently processing 281 / 400\n","Currently processing 282 / 400\n","Currently processing 283 / 400\n","Currently processing 284 / 400\n","Currently processing 285 / 400\n","Currently processing 286 / 400\n","Currently processing 287 / 400\n","Currently processing 288 / 400\n","Currently processing 289 / 400\n","Currently processing 290 / 400\n","Currently processing 291 / 400\n","Currently processing 292 / 400\n","Currently processing 293 / 400\n","Currently processing 294 / 400\n","Currently processing 295 / 400\n","Currently processing 296 / 400\n","Currently processing 297 / 400\n","Currently processing 298 / 400\n","Currently processing 299 / 400\n","Currently processing 300 / 400\n","Cost and accuracy 0.5201600820939266 0.8625\n","Currently processing 301 / 400\n","Currently processing 302 / 400\n","Currently processing 303 / 400\n","Currently processing 304 / 400\n","Currently processing 305 / 400\n","Currently processing 306 / 400\n","Currently processing 307 / 400\n","Currently processing 308 / 400\n","Currently processing 309 / 400\n","Currently processing 310 / 400\n","Currently processing 311 / 400\n","Currently processing 312 / 400\n","Currently processing 313 / 400\n","Currently processing 314 / 400\n","Currently processing 315 / 400\n","Currently processing 316 / 400\n","Currently processing 317 / 400\n","Currently processing 318 / 400\n","Currently processing 319 / 400\n","Currently processing 320 / 400\n","Currently processing 321 / 400\n","Currently processing 322 / 400\n","Currently processing 323 / 400\n","Currently processing 324 / 400\n","Currently processing 325 / 400\n","Currently processing 326 / 400\n","Currently processing 327 / 400\n","Currently processing 328 / 400\n","Currently processing 329 / 400\n","Currently processing 330 / 400\n","Currently processing 331 / 400\n","Currently processing 332 / 400\n","Currently processing 333 / 400\n","Currently processing 334 / 400\n","Currently processing 335 / 400\n","Currently processing 336 / 400\n","Currently processing 337 / 400\n","Currently processing 338 / 400\n","Currently processing 339 / 400\n","Currently processing 340 / 400\n","Currently processing 341 / 400\n","Currently processing 342 / 400\n","Currently processing 343 / 400\n","Currently processing 344 / 400\n","Currently processing 345 / 400\n","Currently processing 346 / 400\n","Currently processing 347 / 400\n","Currently processing 348 / 400\n","Currently processing 349 / 400\n","Currently processing 350 / 400\n","Currently processing 351 / 400\n","Currently processing 352 / 400\n","Currently processing 353 / 400\n","Currently processing 354 / 400\n","Currently processing 355 / 400\n","Currently processing 356 / 400\n","Currently processing 357 / 400\n","Currently processing 358 / 400\n","Currently processing 359 / 400\n","Currently processing 360 / 400\n","Currently processing 361 / 400\n","Currently processing 362 / 400\n","Currently processing 363 / 400\n","Currently processing 364 / 400\n","Currently processing 365 / 400\n","Currently processing 366 / 400\n","Currently processing 367 / 400\n","Currently processing 368 / 400\n","Currently processing 369 / 400\n","Currently processing 370 / 400\n","Currently processing 371 / 400\n","Currently processing 372 / 400\n","Currently processing 373 / 400\n","Currently processing 374 / 400\n","Currently processing 375 / 400\n","Currently processing 376 / 400\n","Currently processing 377 / 400\n","Currently processing 378 / 400\n","Currently processing 379 / 400\n","Currently processing 380 / 400\n","Currently processing 381 / 400\n","Currently processing 382 / 400\n","Currently processing 383 / 400\n","Currently processing 384 / 400\n","Currently processing 385 / 400\n","Currently processing 386 / 400\n","Currently processing 387 / 400\n","Currently processing 388 / 400\n","Currently processing 389 / 400\n","Currently processing 390 / 400\n","Currently processing 391 / 400\n","Currently processing 392 / 400\n","Currently processing 393 / 400\n","Currently processing 394 / 400\n","Currently processing 395 / 400\n","Currently processing 396 / 400\n","Currently processing 397 / 400\n","Currently processing 398 / 400\n","Currently processing 399 / 400\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install datasets\n","from datasets import load_dataset\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","\n","MAX_POOL_SIZE = 5\n","CONVOLUTION_SIZE = 4\n","CONVOLUTION_FILTERS = 2\n","\n","def forward_softmax(x):\n","    \"\"\"\n","    Compute softmax function for a single example.\n","    The shape of the input is of size # num classes.\n","    Important Note: You must be careful to avoid overflow for this function. Functions\n","    like softmax have a tendency to overflow when very large numbers like e^10000 are computed.\n","    You will know that your function is overflow resistent when it can handle input like:\n","    np.array([[10000, 10010, 10]]) without issues.\n","    Args:\n","        x: A 1d numpy float array of shape number_of_classes\n","    Returns:\n","        A 1d numpy float array containing the softmax results of shape  number_of_classes\n","    \"\"\"\n","    x = x - np.max(x,axis=0)\n","    exp = np.exp(x)\n","    s = exp / np.sum(exp,axis=0)\n","    return s\n","\n","def backward_softmax(x, grad_outputs):\n","    \"\"\"\n","    Compute the gradient of the loss with respect to x.\n","    grad_outputs is the gradient of the loss with respect to the outputs of the softmax.\n","    Args:\n","        x: A 1d numpy float array of shape number_of_classes\n","        grad_outputs: A 1d numpy float array of shape number_of_classes\n","    Returns:\n","        A 1d numpy float array of the same shape as x with the derivative of the loss with respect to x\n","    \"\"\"\n","    # *** START CODE HERE ***\n","    \"\"\"\n","    D_{ii} = \\sigma(z)_i(1-\\sigma(z)_i) and D_{ij} = -\\sigma(z)_i\\sigma(z)_j for i\\neq j\n","    In other words, D = -\\sigma(z)\\sigma(z)^T + diag(\\sigma(z))\n","    \"\"\"\n","    sigma_z = forward_softmax(x)\n","    differential = - np.outer(sigma_z, sigma_z)\n","    differential += np.diag(sigma_z)\n","    return differential.T @ grad_outputs\n","    # *** END CODE HERE ***\n","\n","def forward_relu(x):\n","    \"\"\"\n","    Compute the relu function for the input x.\n","    Args:\n","        x: A numpy float array\n","    Returns:\n","        A numpy float array containing the relu results\n","    \"\"\"\n","    x[x<=0] = 0\n","    return x\n","\n","def backward_relu(x, grad_outputs):\n","    \"\"\"\n","    Compute the gradient of the loss with respect to x\n","    Args:\n","        x: A numpy array of arbitrary shape containing the input.\n","        grad_outputs: A numpy array of the same shape of x containing the gradient of the loss with respect\n","            to the output of relu\n","    Returns:\n","        A numpy array of the same shape as x containing the gradients with respect to x.\n","    \"\"\"\n","    # *** START CODE HERE ***\n","    \"\"\"\n","    D_{i,i} = 1(x>0)\n","    \"\"\"\n","    differential = (x > 0).astype(int)\n","    return differential * grad_outputs\n","    # *** END CODE HERE ***\n","\n","def get_initial_params():\n","    \"\"\"\n","    Compute the initial parameters for the neural network.\n","    This function should return a dictionary mapping parameter names to numpy arrays containing\n","    the initial values for those parameters.\n","    There should be four parameters for this model:\n","    W1 is the weight matrix for the convolutional layer\n","    b1 is the bias vector for the convolutional layer\n","    W2 is the weight matrix for the output layers\n","    b2 is the bias vector for the output layer\n","    Weight matrices should be initialized with values drawn from a random normal distribution.\n","    The mean of that distribution should be 0.\n","    The variance of that distribution should be 1/sqrt(n) where n is the number of neurons that\n","    feed into an output for that layer.\n","    Bias vectors should be initialized with zero.\n","    Returns:\n","        A dict mapping parameter names to numpy arrays\n","    \"\"\"\n","    size_after_convolution = 28 - CONVOLUTION_SIZE + 1\n","    size_after_max_pooling = size_after_convolution // MAX_POOL_SIZE\n","    num_hidden = size_after_max_pooling * size_after_max_pooling * CONVOLUTION_FILTERS\n","    return {\n","        'W1': np.random.normal(size = (CONVOLUTION_FILTERS, 1, CONVOLUTION_SIZE, CONVOLUTION_SIZE), scale=1/ math.sqrt(CONVOLUTION_SIZE * CONVOLUTION_SIZE)),\n","        'b1': np.zeros(CONVOLUTION_FILTERS),\n","        'W2': np.random.normal(size = (num_hidden, 10), scale = 1/ math.sqrt(num_hidden)),\n","        'b2': np.zeros(10)\n","    }\n","\n","def forward_convolution(conv_W, conv_b, data):\n","    \"\"\"\n","    Compute the output from a convolutional layer given the weights and data.\n","    conv_W is of the shape (# output channels, # input channels, convolution width, convolution height )\n","    conv_b is of the shape (# output channels)\n","    data is of the shape (# input channels, width, height)\n","    The output should be the result of a convolution and should be of the size:\n","        (# output channels, width - convolution width + 1, height -  convolution height + 1)\n","    Returns:\n","        The output of the convolution as a numpy array\n","    \"\"\"\n","    conv_channels, _, conv_width, conv_height = conv_W.shape\n","    input_channels, input_width, input_height = data.shape\n","    output = np.zeros((conv_channels, input_width - conv_width + 1, input_height - conv_height + 1))\n","    for x in range(input_width - conv_width + 1):\n","        for y in range(input_height - conv_height + 1):\n","            for output_channel in range(conv_channels):\n","                output[output_channel, x, y] = np.sum(\n","                    np.multiply(data[:, x:(x + conv_width), y:(y + conv_height)], conv_W[output_channel, :, :, :])) + conv_b[output_channel]\n","    return output\n","\n","def backward_convolution(conv_W, conv_b, data, output_grad):\n","    \"\"\"\n","    Compute the gradient of the loss with respect to the parameters of the convolution.\n","    See forward_convolution for the sizes of the arguments.\n","    output_grad is the gradient of the loss with respect to the output of the convolution.\n","    Args:\n","        conv_W: of the shape (O, I, W, H)\n","        conv_b: of the shape (O)\n","        data: of the shape (I, X, Y)\n","        output_grad: of the shape (O, X - W + 1, Y - H + 1)\n","    Returns:\n","        A tuple containing 3 gradients.\n","        The first element is the gradient of the loss with respect to the convolution weights\n","        The second element is the gradient of the loss with respect to the convolution bias\n","        The third element is the gradient of the loss with respect to the input data\n","    \"\"\"\n","    # *** START CODE HERE ***\n","    \"\"\"\n","    for 0 <= x < X - W + 1, 0 <= y < Y - H + 1, output[o, x, y] = conv_b[o] + \\sum_{0 <= dx < W, 0 <= dy < H, i}data[i, x+dx, y+dy] * conv_W[o, i, dx, dy]\n","    \\partial J / \\partial conv_W[o, i, dx, dy] = \\sum_{0 <= x < X - W + 1, 0 <= y < Y - H + 1]} output_grad[o, x, y] * data[i, x + dx, y + dy]\n","    \"\"\"\n","    O, I, W, H = conv_W.shape\n","    _, X, Y = data.shape\n","    differential_preprocess = output_grad[None, :, :, :, None, None] * data[:, None, None, None, :, :]\n","    # differential_preprocess.shape == [I, O, X - W + 1, Y - H + 1, X, Y]; output_grad.shape == [O, X - W + 1, Y - H + 1]; data.shape == (I, X, Y)\n","    differential_preprocess2 = np.zeros((I, O, X - W + 1, Y - H + 1, W, H))\n","    for x in range(X - W + 1):\n","      for y in range(Y - H + 1):\n","        differential_preprocess2[:, :, x, y, :, :] = differential_preprocess[:, :, x, y, x:x+W, y:y+H]\n","    # differential_masked[i, o, x, y, w, h] = differential_preprocess[i, o, x, y, x + w, y + h];\n","    differential_conv_W = np.sum(differential_preprocess2, axis=(-4, -3))\n","    differential_conv_W = np.transpose(differential_conv_W, (1, 0, 2, 3))\n","    \"\"\"\n","    \\partial J / \\partial conv_b[o] = \\sum_{0 <= x < X - W + 1, 0 <= y < Y - H + 1} output_grad[o, x, y]\n","    \"\"\"\n","    differential_conv_b = np.sum(output_grad, axis=(-2, -1))\n","    \"\"\"\n","    \\partial J / \\partial data[i, x, y] = \\sum_{o in O, 0 <= dx < \\min\\{x, W\\}, 0 <= dy < \\min\\{y, H\\}}output_grad[o, x - dx, y - dy] * conv_W[o, i, dx, dy]\n","    \"\"\"\n","    differential_data = np.zeros(data.shape)\n","    for x in range(X - W + 1):\n","      for y in range(Y - H + 1):\n","          for o in range(O):\n","            differential_data[:, x:(x + W), y:(y + H)] +=  conv_W[o, :, :, :] * output_grad[o, x, y]\n","    return (differential_conv_W, differential_conv_b, differential_data)\n","    # *** END CODE HERE ***\n","\n","def forward_max_pool(data, pool_width, pool_height):\n","    # this function works only when pool_width and pool_height divide width and height of data\n","    \"\"\"\n","    Compute the output from a max pooling layer given the data and pool dimensions.\n","    The stride length should be equal to the pool size\n","    data is of the shape (# channels, width, height)\n","    The output should be the result of the max pooling layer and should be of size:\n","        (# channels, width // pool_width, height // pool_height)\n","    Returns:\n","        The result of the max pooling layer\n","    \"\"\"\n","    input_channels, input_width, input_height = data.shape\n","    output = np.zeros((input_channels, input_width // pool_width, input_height // pool_height))\n","    for x in range(0, input_width, pool_width):\n","        for y in range(0, input_height, pool_height):\n","            output[:, x // pool_width, y // pool_height] = np.amax(data[:, x:(x + pool_width), y:(y + pool_height)], axis=(1, 2))\n","    return output\n","\n","def backward_max_pool(data, pool_width, pool_height, output_grad):\n","    \"\"\"\n","    Compute the gradient of the loss with respect to the data in the max pooling layer.\n","    data is of the shape (# channels, width, height)\n","    output_grad is of shape (# channels, width // pool_width, height // pool_height)\n","    output_grad is the gradient of the loss with respect to the output of the backward max\n","    pool layer.\n","    Returns:\n","        The gradient of the loss with respect to the data (of same shape as data)\n","    \"\"\"\n","    # *** START CODE HERE ***\n","    differential_data = np.zeros(data.shape)\n","    for x in range(0, data.shape[1], pool_width):\n","      for y in range(0, data.shape[2], pool_height):\n","        for i in range(data.shape[0]):\n","          array = data[i, x:(x + pool_width), y:(y + pool_height)]\n","          global_max_index = np.argmax(array)\n","          row_index, col_index = np.unravel_index(global_max_index, array.shape)\n","          differential_data[i, x + row_index, y + col_index] = output_grad[i, x // pool_width, y // pool_height]\n","    return differential_data\n","    # *** END CODE HERE ***\n","\n","def forward_cross_entropy_loss(probabilities, labels):\n","    \"\"\"\n","    Compute the output from a cross entropy loss layer given the probabilities and labels.\n","    probabilities is of the shape (# classes)\n","    labels is of the shape (# classes)\n","    The output should be a scalar\n","    Returns:\n","        The result of the log loss layer\n","    \"\"\"\n","    result = 0\n","    for i, label in enumerate(labels):\n","        if label == 1:\n","            result += -np.log(probabilities[i])\n","    return result\n","\n","def backward_cross_entropy_loss(probabilities, labels):\n","    \"\"\"\n","    Compute the gradient of the cross entropy loss with respect to the probabilities.\n","    probabilities is of the shape (# classes)\n","    labels is of the shape (# classes)\n","    The output should be the gradient with respect to the probabilities.\n","    Returns:\n","        The gradient of the loss with respect to the probabilities.\n","    \"\"\"\n","    # *** START CODE HERE ***\n","    \"\"\"\n","    differential_probabilities[i] = labels[i] * (- 1 / probabilities[i])\n","    \"\"\"\n","    y_pred = np.clip(probabilities, 1e-12, 1)\n","    return - labels / y_pred\n","    # *** END CODE HERE ***\n","\n","def forward_linear(weights, bias, data):\n","    \"\"\"\n","    Compute the output from a linear layer with the given weights, bias and data.\n","    weights is of the shape (input # features, output # features)\n","    bias is of the shape (output # features)\n","    data is of the shape (input # features)\n","    The output should be of the shape (output # features)\n","    Returns:\n","        The result of the linear layer\n","    \"\"\"\n","    return data.dot(weights) + bias\n","\n","def backward_linear(weights, bias, data, output_grad):\n","    \"\"\"\n","    Compute the gradients of the loss with respect to the parameters of a linear layer.\n","    See forward_linear for information about the shapes of the variables.\n","    output_grad is the gradient of the loss with respect to the output of this layer.\n","    This should return a tuple with three elements:\n","    - The gradient of the loss with respect to the weights\n","    - The gradient of the loss with respect to the bias\n","    - The gradient of the loss with respect to the data\n","    \"\"\"\n","    # *** START CODE HERE ***\n","    \"\"\"\n","    differential_weights[i, j] = data[i] * output_grad[j]\n","    \"\"\"\n","    differential_weights = data[:, None] * output_grad[None, :]\n","    \"\"\"\n","    differential_bias[i] = output_grad[i]\n","    \"\"\"\n","    differential_bias = output_grad\n","    \"\"\"\n","    differential_data[i] = \\sum_j weights[i, j] * output_grad[j]\n","    \"\"\"\n","    differential_data = weights @ output_grad\n","    return (differential_weights, differential_bias, differential_data)\n","    # *** END CODE HERE ***\n","\n","def forward_prop(data, labels, params):\n","    \"\"\"\n","    Implement the forward layer given the data, labels, and params.\n","    Args:\n","        data: A numpy array containing the input (shape is 1 by 28 by 28)\n","        labels: A 1d numpy array containing the labels (shape is 10)\n","        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n","            This numpy array will contain W1, b1, W2 and b2\n","            W1 and b1 represent the weights and bias for the hidden layer of the network\n","            W2 and b2 represent the weights and bias for the output layer of the network\n","    Returns:\n","        A 2 element tuple containing:\n","            1. A numpy array The output (after the softmax) of the output layer\n","            2. The average loss for these data elements\n","    \"\"\"\n","    W1 = params['W1']\n","    b1 = params['b1']\n","    W2 = params['W2']\n","    b2 = params['b2']\n","    first_convolution = forward_convolution(W1, b1, data)\n","    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n","    first_after_relu = forward_relu(first_max_pool)\n","    flattened = np.reshape(first_after_relu, (-1))\n","    logits = forward_linear(W2, b2, flattened)\n","    y = forward_softmax(logits)\n","    cost = forward_cross_entropy_loss(y, labels)\n","    return y, cost\n","\n","def backward_prop(data, labels, params):\n","    \"\"\"\n","    Implement the backward propagation gradient computation step for a neural network\n","    Args:\n","        data: A numpy array containing the input for a single example\n","        labels: A 1d numpy array containing the labels for a single example\n","        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n","            This numpy array will contain W1, b1, W2, and b2\n","            W1 and b1 represent the weights and bias for the convolutional layer\n","            W2 and b2 represent the weights and bias for the output layer of the network\n","    Returns:\n","        A dictionary of strings to numpy arrays where each key represents the name of a weight\n","        and the values represent the gradient of the loss with respect to that weight.\n","        In particular, it should have 4 elements:\n","            W1, W2, b1, and b2\n","    \"\"\"\n","    # *** START CODE HERE ***\n","    W1 = params['W1']\n","    b1 = params['b1']\n","    W2 = params['W2']\n","    b2 = params['b2']\n","    first_convolution = forward_convolution(W1, b1, data)\n","    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n","    first_after_relu = forward_relu(first_max_pool)\n","    flattened = np.reshape(first_after_relu, (-1))\n","    logits = forward_linear(W2, b2, flattened)\n","    probabilities = forward_softmax(logits)\n","    cost = forward_cross_entropy_loss(probabilities, labels)\n","    # print(\"cost = \" + str(cost))\n","    differential_probabilities = backward_cross_entropy_loss(probabilities, labels)\n","    # print(\"differential_probabilities mean=\" + str(np.mean(differential_probabilities)))\n","    differential_logits = backward_softmax(logits, differential_probabilities)\n","    # print(\"differential_logits mean=\" + str(np.mean(differential_logits)))\n","    differential_W2, differential_b2, differential_flattened = backward_linear(W2, b2, flattened, differential_logits)\n","    # print(\"differential_W2 mean=\" + str(np.mean(differential_W2)) + \"; differential_b2 mean=\" + str(np.mean(differential_b2)) + \" differential_flattened mean=\" + str(np.mean(differential_flattened)))\n","    differential_first_after_relu = np.reshape(differential_flattened, first_after_relu.shape)\n","    # print(\"differential_first_after_relu mean=\" + str(np.mean(differential_first_after_relu)))\n","    differential_first_max_pool = backward_relu(first_max_pool, differential_first_after_relu)\n","    # print(\"differential_first_max_pool mean=\" + str(np.mean(differential_first_max_pool)))\n","    differential_first_convolution = backward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE, differential_first_max_pool)\n","    # print(\"differential_first_convolution mean=\" + str(np.mean(differential_first_convolution)))\n","    differential_W1, differential_b1, differential_data = backward_convolution(W1, b1, data, differential_first_convolution)\n","    #print(\"differential_W1 mean=\" + str(np.mean(differential_W1)) + \" differential_b1 mean=\" + str(np.mean(differential_b1)) + \" differential_data mean=\" + str(np.mean(differential_data)))\n","    return {\"W1\" : differential_W1, \"W2\" : differential_W2, \"b1\" : differential_b1, \"b2\" : differential_b2}\n","    # *** END CODE HERE ***\n","\n","def forward_prop_batch(batch_data, batch_labels, params, forward_prop_func):\n","    \"\"\"Apply the forward prop func to every image in a batch\"\"\"\n","    y_array = []\n","    cost_array = []\n","    for item, label in zip(batch_data, batch_labels):\n","        y, cost = forward_prop_func(item, label, params)\n","        y_array.append(y)\n","        cost_array.append(cost)\n","    return np.array(y_array), np.array(cost_array)\n","\n","def gradient_descent_batch(batch_data, batch_labels, learning_rate, params, backward_prop_func):\n","    \"\"\"\n","    Perform one batch of gradient descent on the given training data using the provided learning rate.\n","    This code should update the parameters stored in params.\n","    It should not return anything\n","    Args:\n","        batch_data: A numpy array containing the training data for the batch\n","        train_labels: A numpy array containing the training labels for the batch\n","        learning_rate: The learning rate\n","        params: A dict of parameter names to parameter values that should be updated.\n","        backward_prop_func: A function that follows the backwards_prop API\n","    Returns: This function returns nothing.\n","    \"\"\"\n","    total_grad = {}\n","    for i in range(batch_data.shape[0]):\n","        grad = backward_prop_func(\n","            batch_data[i, :, :],\n","            batch_labels[i, :],\n","            params)\n","        for key, value in grad.items():\n","            if key not in total_grad:\n","                total_grad[key] = np.zeros(value.shape)\n","            total_grad[key] += value\n","    for key, value in total_grad.items():\n","      total_grad[key] = total_grad[key] / batch_data.shape[0]\n","    params['W1'] = params['W1'] - learning_rate * total_grad['W1']\n","    # print('W1 grad mean=' + str(np.mean(total_grad['W1'])))\n","    params['W2'] = params['W2'] - learning_rate * total_grad['W2']\n","    # print('W2 grad mean=' + str(np.mean(total_grad['W2'])))\n","    params['b1'] = params['b1'] - learning_rate * total_grad['b1']\n","    # print('b1 grad mean=' + str(np.mean(total_grad['b1'])))\n","    params['b2'] = params['b2'] - learning_rate * total_grad['b2']\n","    # print('b2 grad mean=' + str(np.mean(total_grad['b2'])))\n","    # This function does not return anything\n","    return\n","\n","def nn_train(\n","    train_data, train_labels, dev_data, dev_labels,\n","    get_initial_params_func, forward_prop_func, backward_prop_func,\n","    learning_rate=5.0, batch_size=16, num_batches=400):\n","    m = train_data.shape[0]\n","    params = get_initial_params_func()\n","    cost_dev = []\n","    accuracy_dev = []\n","    for batch in range(num_batches):\n","        print('Currently processing {} / {}'.format(batch, num_batches))\n","        batch_data = train_data[batch * batch_size:(batch + 1) * batch_size, :, :, :]\n","        batch_labels = train_labels[batch * batch_size: (batch + 1) * batch_size, :]\n","        if batch % 100 == 0:\n","            output, cost = forward_prop_batch(dev_data, dev_labels, params, forward_prop_func)\n","            cost_dev.append(sum(cost) / len(cost))\n","            accuracy_dev.append(compute_accuracy(output, dev_labels))\n","            print('Cost and accuracy', cost_dev[-1], accuracy_dev[-1])\n","        gradient_descent_batch(batch_data, batch_labels,\n","            learning_rate, params, backward_prop_func)\n","    return params, cost_dev, accuracy_dev\n","\n","def nn_test(data, labels, params):\n","    output, cost = forward_prop(data, labels, params)\n","    accuracy = compute_accuracy(output, labels)\n","    return accuracy\n","\n","def compute_accuracy(output, labels):\n","    correct_output = np.argmax(output,axis=1)\n","    correct_labels = np.argmax(labels,axis=1)\n","    is_correct = [a == b for a,b in zip(correct_output, correct_labels)]\n","    accuracy = sum(is_correct) * 1. / labels.shape[0]\n","    return accuracy\n","\n","def one_hot_labels(labels):\n","    one_hot_labels = np.zeros((labels.size, 10))\n","    one_hot_labels[np.arange(labels.size),labels.astype(int)] = 1\n","    return one_hot_labels\n","\n","#def read_data(images_file, labels_file):\n","#    x = np.loadtxt(images_file, delimiter=',')\n","#    y = np.loadtxt(labels_file, delimiter=',')\n","#    x = np.reshape(x, (x.shape[0], 1, 28, 28))\n","#    return x, y\n","\n","def run_train(all_data, all_labels, backward_prop_func):\n","    params, cost_dev, accuracy_dev = nn_train(\n","        all_data['train'], all_labels['train'],\n","        all_data['dev'], all_labels['dev'],\n","        get_initial_params, forward_prop, backward_prop_func,\n","        learning_rate=3e-1, batch_size=16, num_batches=400\n","    )\n","    t = np.arange(400 // 100)\n","    fig, (ax1, ax2) = plt.subplots(2, 1)\n","    ax1.plot(t, cost_dev, 'b')\n","    ax1.set_xlabel('time')\n","    ax1.set_ylabel('loss')\n","    ax1.set_title('Training curve')\n","    ax2.plot(t, accuracy_dev, 'b')\n","    ax2.set_xlabel('time')\n","    ax2.set_ylabel('accuracy')\n","    fig.savefig('/content/drive/MyDrive/cs229/problem-sets/PS4/output/train.png')\n","\n","def main():\n","    np.random.seed(100)\n","    ds = load_dataset(\"ylecun/mnist\")\n","    train_dataset = ds['train']\n","    train_data = np.array([np.array(sample['image']) for sample in train_dataset])\n","    train_data = np.reshape(train_data, (train_data.shape[0], 1, 28, 28))\n","    train_labels = np.array([np.array(sample['label']) for sample in train_dataset])\n","    # train_data, train_labels = read_data('/content/drive/MyDrive/cs229/problem-sets/PS4/data/images_train.csv', '/content/drive/MyDrive/cs229/problem-sets/PS4/data/labels_train.csv')\n","    train_labels = one_hot_labels(train_labels)\n","    p = np.random.permutation(60000)\n","    train_data = train_data[p,:]\n","    train_labels = train_labels[p,:]\n","    dev_data = train_data[0:400,:]\n","    dev_labels = train_labels[0:400,:]\n","    train_data = train_data[400:,:]\n","    train_labels = train_labels[400:,:]\n","    mean = np.mean(train_data)\n","    std = np.std(train_data)\n","    train_data = (train_data - mean) / std\n","    dev_data = (dev_data - mean) / std\n","    all_data = {\n","        'train': train_data,\n","        'dev': dev_data,\n","    }\n","    all_labels = {\n","        'train': train_labels,\n","        'dev': dev_labels,\n","    }\n","    run_train(all_data, all_labels, backward_prop)\n","\n","if __name__ == '__main__':\n","\n","    main()"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMS5v83/lA1+VkvXqIKVmMh"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}