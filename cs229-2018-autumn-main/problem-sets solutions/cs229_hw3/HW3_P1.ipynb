{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP2j6Zq+RNn6btMR0ezBF4j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["For an input $x^{i}$, let $z^{[1]}=MM_{W^{[1]},b^{[1]}}(x)$, $h=\\sigma(z^{[1]})$, $z^{[2]}=MM_{W^{[2]},b^{[2]}}(h)$, $o=\\sigma(z^{[2]})$, and $J=\\ell_{MSE}(o,y)$. \\\\\n","Then, $\\frac{\\partial J}{\\partial o}=\\frac{2}{m}(o-y)$, $\\frac{\\partial J}{\\partial z^{[2]}}=\\mathcal{B}[\\sigma,z^{[2]}](\\frac{\\partial J}{\\partial o})=o(1-o)\\frac{\\partial J}{\\partial o}$, $\\frac{\\partial J}{h}=\\mathcal{B}[MM,](\\frac{\\partial J}{\\partial z^{[2]}})=(W^{[2]})^T\\frac{\\partial J}{\\partial z^{[2]}}$, $\\frac{\\partial J}{\\partial z^{[1]}}=\\mathcal{B}(\\sigma,z^{[1]})(\\frac{\\partial J}{\\partial })=\\sigma(z^{[1]})\\odot(1-\\sigma(z^{[1]}))\\odot \\frac{\\partial J}{\\partial }$, and $\\frac{\\partial J}{\\partial W_{1,2}^{[1]}}=\\mathcal{B}[MM,W^{[1]}](\\frac{\\partial J}{\\partial z^{[1]}})_{1,2}=\\frac{\\partial J}{\\partial z^{[1]}}(z^{[1]})^T_{1,2}=x^{[1]}_2\\frac{\\partial J}{\\partial z^{[1]}_1}$.\\\\\n","So, $\\frac{\\partial J}{\\partial w^{[1]}_{1,2}}=x_2\\frac{\\partial J}{\\partial z_1^{[1]}}$$=x_2\\sigma(z^{[1]}_1)(1-\\sigma(z^{[1]}_1))\\frac{\\partial J}{\\partial _1}$$=x_2\\sigma(z^{[1]}_1)(1-\\sigma(z^{[1]}_1))(W^{[2]}_{11}\\frac{\\partial J}{\\partial z^{[2]}})$$=x_2\\sigma(z^{[1]}_1)(1-\\sigma(z^{[1]}_1))W^{[2]}_{11}\\sigma(z^{[2]})(1-\\sigma(z^{[2]}))\\frac{2}{m}(o-y)$ for a single pair of training data $(x,y)$ in the batch.\\\\\n","So, the gradient descent update to $w_{1,2}^{[1]}$ for the entire batch of training data equals $w_{1,2}^{[1]}\\leftarrow w_{1,2}^{[1]}-\\alpha\\frac{\\partial J}{\\partial w_{1,2}^{[1]}}$\n","\n","$=w_{1,2}^{[1]}-\\alpha\\frac{2}{m}\\sum_{i=1}^mx_2^{(i)}h^{(i)}_1(1-h^{(i)}_1)W_{11}^{[2]}o^{(i)}(1-o^{(i)})(o^{(i)}-y^{(i)})$."],"metadata":{"id":"L9tpknP7ZKnY"}},{"cell_type":"markdown","source":["1 b)\n","\n","It is possible. Positive examples lie in a triangle described by $x_1\\leq c_1$, $x_2\\leq c_2$, and $x_1+sx_2\\geq c_3$ for some $s, c_1, c_2, c_3>0$. Let $h_1$ be $1$ iff $x_1\\leq c_1$, $h_2$ be $1$ iff $x_2\\leq c_2$, and $h_3$ be $1$ iff $x_1+sx_2\\geq c_3$. Let $o$ be $1$ iff $h_1=1$ or $h_2=1$ or $h_3=1$.\n","\n","$h_1$ could be computed by setting $w_{1,1}^{[1]}=-1$ and $w_{0,1}^{[1]}=c_1$. $h_2$ could be computed by setting $w_{2,2}^{[1]}=-1$ and $w_{0,2}^{[1]}=c_2$. $h_3$ could be computed by setting $w_{1,3}^{[1]}=1$, $w_{2,3}^{[1]}=s$, and $w_{0,3}^{[1]}=-c_3$. $o$ could be computed by setting $w_1^{[2]}=1$, $w_2^{[2]}=1$, and $w_3^{[2]}=1$, and $w_0^{[2]}=0$.\n","\n","A feasible set of $s,c_1,c_2,c_3$ could be $c_1=0.5$, $c_2=0.5$, $s=1$, and $c_3=4$."],"metadata":{"id":"bV02ipEGs8lk"}},{"cell_type":"markdown","source":["1 c)\n","\n","It is not possible. If the activation functions of the hidden layers are linear, then $o$ outputs $1$ for samples on one side of line and $1$ for samples on another."],"metadata":{"id":"70KDyrvPAFES"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"86wDzS4OQO6D"},"outputs":[],"source":["import json\n","\n","def example_weights():\n","    \"\"\"This is an example function that returns weights.\n","    Use this function as a template for optimal_step_weights and optimal_sigmoid_weights.\n","    You do not need to modify this class for this assignment.\n","    \"\"\"\n","    w = {}\n","\n","    w['hidden_layer_0_1'] = 0\n","    w['hidden_layer_1_1'] = 0\n","    w['hidden_layer_2_1'] = 0\n","    w['hidden_layer_0_2'] = 0\n","    w['hidden_layer_1_2'] = 0\n","    w['hidden_layer_2_2'] = 0\n","    w['hidden_layer_0_3'] = 0\n","    w['hidden_layer_1_3'] = 0\n","    w['hidden_layer_2_3'] = 0\n","\n","    w['output_layer_0'] = 0\n","    w['output_layer_1'] = 0\n","    w['output_layer_2'] = 0\n","    w['output_layer_3'] = 0\n","\n","    return w\n","\n","\n","def optimal_step_weights():\n","    \"\"\"Return the optimal weights for the neural network with a step activation function.\n","\n","    This function will not be graded if there are no optimal weights.\n","    See the PDF for instructions on what each weight represents.\n","\n","    The hidden layer weights are notated by [1] on the problem set and\n","    the output layer weights are notated by [2].\n","\n","    This function should return a dict with elements for each weight, see example_weights above.\n","\n","    \"\"\"\n","    w = example_weights()\n","\n","    # *** START CODE HERE ***\n","    w['hidden_layer_0_1'] = 0.5\n","    w['hidden_layer_1_1'] = -1\n","    w['hidden_layer_2_1'] = 0\n","    w['hidden_layer_0_2'] = 0.5\n","    w['hidden_layer_1_2'] = 0\n","    w['hidden_layer_2_2'] = -1\n","    w['hidden_layer_0_3'] = -4\n","    w['hidden_layer_1_3'] = 1\n","    w['hidden_layer_2_3'] = 1\n","\n","    w['output_layer_0'] = 0\n","    w['output_layer_1'] = 1\n","    w['output_layer_2'] = 1\n","    w['output_layer_3'] = 1\n","    # *** END CODE HERE ***\n","\n","    return w\n","\n","def optimal_linear_weights():\n","    \"\"\"Return the optimal weights for the neural network with a linear activation function for the hidden units.\n","\n","    This function will not be graded if there are no optimal weights.\n","    See the PDF for instructions on what each weight represents.\n","\n","    The hidden layer weights are notated by [1] on the problem set and\n","    the output layer weights are notated by [2].\n","\n","    This function should return a dict with elements for each weight, see example_weights above.\n","\n","    \"\"\"\n","    w = example_weights()\n","\n","    # *** START CODE HERE ***\n","    w['hidden_layer_0_1'] = 0.5\n","    w['hidden_layer_1_1'] = -1\n","    w['hidden_layer_2_1'] = 0\n","    w['hidden_layer_0_2'] = 0.5\n","    w['hidden_layer_1_2'] = 0\n","    w['hidden_layer_2_2'] = -1\n","    w['hidden_layer_0_3'] = -4\n","    w['hidden_layer_1_3'] = 1\n","    w['hidden_layer_2_3'] = 1\n","\n","    w['output_layer_0'] = 0\n","    w['output_layer_1'] = 1\n","    w['output_layer_2'] = 1\n","    w['output_layer_3'] = 1\n","    # *** END CODE HERE ***\n","\n","    return w\n","\n","if __name__ == \"__main__\":\n","    step_weights = optimal_step_weights()\n","\n","    with open('output/step_weights', 'w') as f:\n","        json.dump(step_weights, f)\n","\n","    linear_weights = optimal_linear_weights()\n","\n","    with open('output/linear_weights', 'w') as f:\n","        json.dump(linear_weights, f)"]}]}