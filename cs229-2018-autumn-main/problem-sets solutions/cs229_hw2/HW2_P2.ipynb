{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNfsmezZl+bTTXPlEnZXTxP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["2 a)\n","\n","Training algorithm for logistic regression maximizes log-likelihood. Using first-order conditions, setting $\\sum_{i=1}^m (y-h_\\theta(x))=\\sum_{i=1}^m (y-h_\\theta(x))x_{n+1}=\\partial \\ell(\\theta)/\\partial\\theta_{n+1}=0$ by first order conditions, so a perfectly trained logistic regression model is perfectly calibrated on $(0,1)$.\n","\n","2 b)\n","\n","If $x_1\\neq x_2$ for two training points $(x_1,1)$ and $(x_2,0)$, then there is a binary classification model $f$ that predicts $f(x1)=0$ and $f(x2)=1$ because $I_{a,b}$ is always empty and average of model outputs and fraction of positives are always vacuously equal to one another.\n","\n","If $x_1\\neq x_2$ for two samples $(x_1,1)$ and $(x_2,0)$, the samples are linearly separable. So, there is a binary classification model $f$ that predicts $h_\\theta(x_1)>1/2$ and $h_\\theta(x_2)<1/2$, or equivalently being perfectly accurate on the training set. However, the model is not not perfectly calibrated on $(0,1/2)$.\n","\n","2 c)\n","\n","Including L2-regularization would change the first-order condition for $\\theta_{n+1}$ and distort calibration in the interval $(0,1)$."],"metadata":{"id":"xfrWykjIM3_u"}}]}